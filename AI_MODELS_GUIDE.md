# Ø¯Ù„ÙŠÙ„ Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
# AI Models Integration Guide

## ğŸ“‹ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© / Overview

Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙŠÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¬Ø¯ÙŠØ¯Ø© Ø¥Ù„Ù‰ Ù…Ù†ØµØ© VEO7. ÙŠØªØ¶Ù…Ù† Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.

This guide explains how to add new AI models to the VEO7 platform, including technical and architectural steps for proper integration.

---

## ğŸ—ï¸ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© / Architecture

### Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª / Folder Structure
```
backend/
â”œâ”€â”€ ai_models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_model.py          # Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ù†Ù…Ø§Ø°Ø¬
â”‚   â”œâ”€â”€ sadtalker_model.py     # Ù†Ù…ÙˆØ°Ø¬ SadTalker
â”‚   â”œâ”€â”€ wav2lip_model.py       # Ù†Ù…ÙˆØ°Ø¬ Wav2Lip
â”‚   â”œâ”€â”€ realesrgan_model.py    # Ù†Ù…ÙˆØ°Ø¬ Real-ESRGAN
â”‚   â””â”€â”€ your_new_model.py      # Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯
â”œâ”€â”€ models/                    # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
â”‚   â”œâ”€â”€ sadtalker/
â”‚   â”œâ”€â”€ wav2lip/
â”‚   â”œâ”€â”€ realesrgan/
â”‚   â””â”€â”€ your_model/
â””â”€â”€ services/
    â””â”€â”€ ai_service.py          # Ø®Ø¯Ù…Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
```

---

## ğŸ”§ Ø¥Ø¶Ø§ÙØ© Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ / Adding a New Model

### 1. Ø¥Ù†Ø´Ø§Ø¡ ÙØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Create Model Class

```python
# backend/ai_models/your_new_model.py

import torch
import numpy as np
from typing import Dict, Any, Optional
from .base_model import BaseAIModel

class YourNewModel(BaseAIModel):
    """
    Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    New AI model implementation
    """
    
    def __init__(self, model_path: str, device: str = "cuda"):
        super().__init__(model_path, device)
        self.model_name = "your_new_model"
        self.model_version = "1.0.0"
        
    def load_model(self) -> bool:
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£ÙˆØ²Ø§Ù†Ù‡
        Load the model and its weights
        """
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‡Ù†Ø§
            # Load your model here
            self.model = torch.load(
                self.model_path, 
                map_location=self.device
            )
            self.model.eval()
            self.is_loaded = True
            return True
        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
            return False
    
    def preprocess_input(self, input_data: Dict[str, Any]) -> Any:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
        Preprocess input data
        """
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        # Process data according to model requirements
        processed_data = input_data
        return processed_data
    
    def inference(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        Run model inference
        """
        if not self.is_loaded:
            raise RuntimeError("Model not loaded")
        
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
            processed_input = self.preprocess_input(input_data)
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            with torch.no_grad():
                output = self.model(processed_input)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            result = self.postprocess_output(output)
            
            return {
                "success": True,
                "result": result,
                "model_info": {
                    "name": self.model_name,
                    "version": self.model_version
                }
            }
            
        except Exception as e:
            self.logger.error(f"Inference failed: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def postprocess_output(self, output: Any) -> Any:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        Postprocess model output
        """
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
        # Process output as needed
        return output
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        Get model information
        """
        return {
            "name": self.model_name,
            "version": self.model_version,
            "device": self.device,
            "is_loaded": self.is_loaded,
            "memory_usage": self.get_memory_usage(),
            "supported_formats": ["jpg", "png", "mp4"],  # Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            "description": "ÙˆØµÙ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯"
        }
```

### 2. ØªØ­Ø¯ÙŠØ« Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ / Update AI Service

```python
# backend/services/ai_service.py

from ai_models.your_new_model import YourNewModel

class AIService:
    def __init__(self):
        # ... Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
        self.your_new_model = None
    
    def initialize_your_new_model(self) -> bool:
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯"""
        try:
            model_path = os.getenv("YOUR_MODEL_PATH", "./models/your_model")
            device = os.getenv("DEVICE", "cuda" if torch.cuda.is_available() else "cpu")
            
            self.your_new_model = YourNewModel(model_path, device)
            success = self.your_new_model.load_model()
            
            if success:
                logger.info("Your new model initialized successfully")
            else:
                logger.error("Failed to initialize your new model")
            
            return success
        except Exception as e:
            logger.error(f"Error initializing your new model: {e}")
            return False
    
    def process_with_your_model(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯"""
        if not self.your_new_model or not self.your_new_model.is_loaded:
            return {
                "success": False,
                "error": "Your new model not available"
            }
        
        return self.your_new_model.inference(input_data)
```

### 3. Ø¥Ø¶Ø§ÙØ© API Endpoints / Add API Endpoints

```python
# backend/main.py

@app.post("/api/ai-models/your-new-model")
async def process_with_your_model(
    request: YourModelRequest,
    current_user: User = Depends(get_current_user)
):
    """
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    Process data with your new model
    """
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        input_data = {
            "param1": request.param1,
            "param2": request.param2,
            # ... Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        }
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        result = ai_service.process_with_your_model(input_data)
        
        if result["success"]:
            return {
                "success": True,
                "result": result["result"],
                "processing_time": result.get("processing_time", 0)
            }
        else:
            raise HTTPException(
                status_code=500,
                detail=result["error"]
            )
            
    except Exception as e:
        logger.error(f"Error in your new model endpoint: {e}")
        raise HTTPException(
            status_code=500,
            detail="Internal server error"
        )

# Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
class YourModelRequest(BaseModel):
    param1: str
    param2: Optional[int] = None
    settings: Optional[Dict[str, Any]] = {}
```

### 4. Ø¥Ø¶Ø§ÙØ© ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… / Add Frontend Interface

```typescript
// frontend/components/ai-studio/YourNewModelPanel.tsx

import React, { useState } from 'react';
import { useAuth } from '@/contexts/AuthContext';
import { useLanguage } from '@/contexts/LanguageContext';

interface YourModelPanelProps {
  onProcessingStart: () => void;
  onProcessingComplete: (result: any) => void;
  onError: (error: string) => void;
}

export const YourNewModelPanel: React.FC<YourModelPanelProps> = ({
  onProcessingStart,
  onProcessingComplete,
  onError
}) => {
  const { user } = useAuth();
  const { language, t } = useLanguage();
  const [isProcessing, setIsProcessing] = useState(false);
  const [settings, setSettings] = useState({
    param1: '',
    param2: 1,
  });

  const handleProcess = async () => {
    if (!user) {
      onError(t('auth.loginRequired'));
      return;
    }

    setIsProcessing(true);
    onProcessingStart();

    try {
      const response = await fetch('/api/ai-models/your-new-model', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${user.token}`
        },
        body: JSON.stringify(settings)
      });

      const result = await response.json();

      if (result.success) {
        onProcessingComplete(result.result);
      } else {
        onError(result.error || t('errors.processingFailed'));
      }
    } catch (error) {
      onError(t('errors.networkError'));
    } finally {
      setIsProcessing(false);
    }
  };

  return (
    <div className="bg-white dark:bg-gray-800 rounded-lg p-6 shadow-lg">
      <h3 className="text-xl font-bold mb-4">
        {t('aiStudio.yourNewModel.title')}
      </h3>
      
      {/* ÙˆØ§Ø¬Ù‡Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ */}
      <div className="space-y-4">
        <div>
          <label className="block text-sm font-medium mb-2">
            {t('aiStudio.yourNewModel.param1')}
          </label>
          <input
            type="text"
            value={settings.param1}
            onChange={(e) => setSettings({...settings, param1: e.target.value})}
            className="w-full p-3 border rounded-lg"
          />
        </div>
        
        <div>
          <label className="block text-sm font-medium mb-2">
            {t('aiStudio.yourNewModel.param2')}
          </label>
          <input
            type="number"
            value={settings.param2}
            onChange={(e) => setSettings({...settings, param2: parseInt(e.target.value)})}
            className="w-full p-3 border rounded-lg"
          />
        </div>
        
        <button
          onClick={handleProcess}
          disabled={isProcessing}
          className="w-full bg-purple-600 text-white py-3 rounded-lg hover:bg-purple-700 disabled:opacity-50"
        >
          {isProcessing ? t('common.processing') : t('aiStudio.yourNewModel.process')}
        </button>
      </div>
    </div>
  );
};
```

### 5. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ±Ø¬Ù…Ø§Øª / Add Translations

```json
// frontend/locales/ar.json
{
  "aiStudio": {
    "yourNewModel": {
      "title": "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯",
      "param1": "Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø£ÙˆÙ„",
      "param2": "Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„Ø«Ø§Ù†ÙŠ",
      "process": "Ù…Ø¹Ø§Ù„Ø¬Ø©",
      "description": "ÙˆØµÙ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯"
    }
  }
}

// frontend/locales/en.json
{
  "aiStudio": {
    "yourNewModel": {
      "title": "Your New Model",
      "param1": "Parameter 1",
      "param2": "Parameter 2",
      "process": "Process",
      "description": "Description of your new model"
    }
  }
}
```

---

## ğŸ§ª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± / Testing

### 1. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Model Testing

```python
# tests/test_your_new_model.py

import pytest
from ai_models.your_new_model import YourNewModel

class TestYourNewModel:
    def setup_method(self):
        self.model = YourNewModel("./models/your_model", "cpu")
    
    def test_model_loading(self):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        success = self.model.load_model()
        assert success == True
        assert self.model.is_loaded == True
    
    def test_inference(self):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        self.model.load_model()
        
        input_data = {
            "param1": "test_value",
            "param2": 1
        }
        
        result = self.model.inference(input_data)
        assert result["success"] == True
        assert "result" in result
    
    def test_model_info(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        info = self.model.get_model_info()
        assert "name" in info
        assert "version" in info
        assert info["name"] == "your_new_model"
```

### 2. Ø§Ø®ØªØ¨Ø§Ø± API / API Testing

```python
# tests/test_api_your_model.py

import pytest
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_your_model_endpoint():
    """Ø§Ø®ØªØ¨Ø§Ø± endpoint Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯"""
    response = client.post(
        "/api/ai-models/your-new-model",
        json={
            "param1": "test",
            "param2": 1
        },
        headers={"Authorization": "Bearer test_token"}
    )
    
    assert response.status_code == 200
    data = response.json()
    assert data["success"] == True
```

---

## ğŸ“¦ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª / Dependencies

### Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© / Add Required Dependencies

```txt
# backend/requirements_your_model.txt
your-model-library==1.0.0
additional-dependency==2.0.0
```

```bash
# ØªØ«Ø¨ÙŠØª Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
pip install -r backend/requirements_your_model.txt
```

---

## ğŸ”§ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª / Configuration

### Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© / Environment Variables

```bash
# .env
YOUR_MODEL_PATH=./models/your_model
YOUR_MODEL_ENABLED=true
YOUR_MODEL_BATCH_SIZE=4
YOUR_MODEL_MAX_MEMORY=8GB
```

### Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Model Configuration

```python
# backend/config/your_model_config.py

YOUR_MODEL_CONFIG = {
    "model_path": "./models/your_model",
    "device": "cuda",
    "batch_size": 4,
    "max_memory": "8GB",
    "preprocessing": {
        "resize": (512, 512),
        "normalize": True
    },
    "postprocessing": {
        "format": "mp4",
        "quality": "high"
    }
}
```

---

## ğŸ“š Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª / Best Practices

### 1. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© / Memory Management
```python
def cleanup_memory(self):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ©
    del self.temp_variables
```

### 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ / Error Handling
```python
try:
    result = self.model.inference(input_data)
except torch.cuda.OutOfMemoryError:
    self.logger.error("GPU memory exhausted")
    # ØªØ¨Ø¯ÙŠÙ„ Ø¥Ù„Ù‰ CPU
    self.device = "cpu"
    self.model.to(self.device)
except Exception as e:
    self.logger.error(f"Unexpected error: {e}")
    raise
```

### 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Input Validation
```python
def validate_input(self, input_data: Dict[str, Any]) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø©"""
    required_fields = ["param1", "param2"]
    
    for field in required_fields:
        if field not in input_data:
            raise ValueError(f"Missing required field: {field}")
    
    return True
```

### 4. Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„Ø³Ø¬Ù„Ø§Øª / Monitoring and Logging
```python
import time
from functools import wraps

def monitor_performance(func):
    """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        processing_time = end_time - start_time
        logger.info(f"Model processing time: {processing_time:.2f}s")
        
        return result
    return wrapper
```

---

## ğŸš€ Ø§Ù„Ù†Ø´Ø± / Deployment

### 1. Ø¥Ø¹Ø¯Ø§Ø¯ Docker / Docker Setup
```dockerfile
# Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Dockerfile
COPY models/your_model /app/models/your_model
RUN pip install -r requirements_your_model.txt
```

### 2. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¥Ù†ØªØ§Ø¬ / Production Testing
```bash
# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬
python -m pytest tests/test_your_new_model.py -v
```

---

## ğŸ“ Ø§Ù„Ø¯Ø¹Ù… / Support

Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø§Ø°Ø¬ Ø¬Ø¯ÙŠØ¯Ø©:
- **Ø§Ù„ØªÙˆØ«ÙŠÙ‚**: Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ `ai_models/`
- **GitHub Issues**: Ø£Ù†Ø´Ø¦ issue Ø¬Ø¯ÙŠØ¯ Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
- **Discord**: Ø§Ù†Ø¶Ù… Ø¥Ù„Ù‰ Ù‚Ù†Ø§Ø© Ø§Ù„Ù…Ø·ÙˆØ±ÙŠÙ†

For help with adding new models:
- **Documentation**: Check existing examples in `ai_models/`
- **GitHub Issues**: Create a new issue with model details
- **Discord**: Join the developers channel

---

**ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© ÙØ±ÙŠÙ‚ VEO7 ğŸ’œ**